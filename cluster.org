* BOARD
** [[https://github.com/cncf/apisnoop/projects/29#column-8865828][prow.cncf.io-TODO]]
*** TODO ghproxy
*** TODO plank
    [[https://github.com/kubernetes/test-infra/tree/0bde37ecf717799c2953131192c997373ffe976d/prow/cmd/plank][Plank]]
    [[https://github.com/kubernetes/test-infra/blob/0bde37ecf717799c2953131192c997373ffe976d/prow/pod-utilities.md][Pod Utilities]]
*** DONE tide
    CLOSED: [2020-05-14 Thu 03:21]
    -  https://prow.k8s.io/tide
    So here we see Merge Requirements and repos that have incoming PRs
    - cf. http://prow.cncf.io/tide
    - [[https://github.com/kubernetes/test-infra/blob/0bde37ecf717799c2953131192c997373ffe976d/config/prow/config.yaml#L351][Tide Config for test-infra]]

** [[https://github.com/cncf/apisnoop/projects/29#column-8865858][cncf/k8s-conformance-TODO]]
*** TODO confirm version of k8s being tested matched foldera
*** TODO generate list of tests required for a specific version
*** TODO generate list of tests submitted as run by a PR
*** TODO add blocking tag if any requeired tests not run
*** TODO comment with list of missing tests
* strat
** get presubmits job results showing up as comments
** optional
** required
** simple presubmit job (cat/dog) counts something [[https://github.com/cncf/k8s-conformance][cncf/k8s-conformance]]

* Setup a cluster

Used *eksctl* though long term, maybe we use cluster-api and [[https://github.com/aws/aws-service-operator-k8s/blob/master/docs/background.md#custom-controllers-and-operators-in-aws][aws-service-operation-k8s]]

#+begin_src  shell
aws eks list-clusters
#+end_src

#+RESULTS:
#+begin_example
{
    "clusters": [
        "prow-dev"
    ]
}
#+end_example
* loading secrets
** github-hmac / hook
 #+begin_src shell
   kubectl delete secret hmac-token
   kubectl create secret generic hmac-token --from-file=hmac=.secret-hook
 #+end_src

 #+RESULTS:
 #+begin_example
 secret/hmac-token created
 #+end_example

** github-oauth
 #+begin_src shell
   kubectl delete secret oauth-token
   kubectl create secret generic oauth-token --from-file=oauth=.secret-oauth
 #+end_src

 #+RESULTS:
 #+begin_example
 secret "oauth-token" deleted
 secret/oauth-token created
 #+end_example

* prow components manifst
** cluster/starter.yaml
https://github.com/kubernetes/test-infra/blob/master/prow/getting_started_deploy.md#add-the-prow-components-to-the-cluster
#+begin_src shell :dir "~/prow-config"
  kubectl apply -f manifests/starter.yaml
#+end_src

#+RESULTS:
#+begin_example
configmap/plugins unchanged
configmap/config unchanged
customresourcedefinition.apiextensions.k8s.io/prowjobs.prow.k8s.io unchanged
deployment.apps/hook configured
service/hook unchanged
deployment.apps/plank unchanged
deployment.apps/sinker unchanged
deployment.apps/deck unchanged
service/deck unchanged
deployment.apps/horologium unchanged
deployment.apps/tide unchanged
service/tide unchanged
ingress.extensions/ing unchanged
deployment.apps/statusreconciler unchanged
namespace/test-pods unchanged
serviceaccount/deck unchanged
rolebinding.rbac.authorization.k8s.io/deck unchanged
rolebinding.rbac.authorization.k8s.io/deck unchanged
role.rbac.authorization.k8s.io/deck unchanged
role.rbac.authorization.k8s.io/deck unchanged
serviceaccount/horologium unchanged
role.rbac.authorization.k8s.io/horologium unchanged
rolebinding.rbac.authorization.k8s.io/horologium unchanged
serviceaccount/plank unchanged
role.rbac.authorization.k8s.io/plank unchanged
role.rbac.authorization.k8s.io/plank unchanged
rolebinding.rbac.authorization.k8s.io/plank unchanged
rolebinding.rbac.authorization.k8s.io/plank unchanged
serviceaccount/sinker unchanged
role.rbac.authorization.k8s.io/sinker unchanged
role.rbac.authorization.k8s.io/sinker unchanged
rolebinding.rbac.authorization.k8s.io/sinker unchanged
rolebinding.rbac.authorization.k8s.io/sinker unchanged
serviceaccount/hook unchanged
role.rbac.authorization.k8s.io/hook unchanged
rolebinding.rbac.authorization.k8s.io/hook unchanged
serviceaccount/tide unchanged
role.rbac.authorization.k8s.io/tide unchanged
rolebinding.rbac.authorization.k8s.io/tide unchanged
serviceaccount/statusreconciler unchanged
role.rbac.authorization.k8s.io/statusreconciler unchanged
rolebinding.rbac.authorization.k8s.io/statusreconciler unchanged
#+end_example

* components
** services
#+begin_src shell
  kubectl get services
#+end_src

#+RESULTS:
#+begin_example
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
deck         NodePort    10.100.33.217    <none>        80:32096/TCP     12h
hook         NodePort    10.100.156.63    <none>        8888:31025/TCP   12h
kubernetes   ClusterIP   10.100.0.1       <none>        443/TCP          13h
tide         NodePort    10.100.132.246   <none>        80:31081/TCP     12h
#+end_example

** pods
#+begin_src shell
  kubectl get pods
#+end_src

#+RESULTS:
#+begin_example
NAME                               READY   STATUS    RESTARTS   AGE
deck-7c6d46b4f7-8nj26              1/1     Running   0          12h
deck-7c6d46b4f7-p7lws              1/1     Running   0          12h
hook-5f4db6758f-g7dzs              1/1     Running   0          12h
hook-5f4db6758f-rcg87              1/1     Running   0          12h
horologium-54f95c4dc4-5km7m        1/1     Running   0          12h
plank-7cf6bf5cb6-9njpm             1/1     Running   0          12h
sinker-ddf8cbcb6-fxbzb             1/1     Running   0          12h
statusreconciler-b946855cf-jt89w   1/1     Running   0          12h
tide-66b57f5ccf-wsns5              1/1     Running   0          12h
#+end_example

** deployment

#+begin_src shell
  kubectl get deployments
#+end_src

#+RESULTS:
#+begin_example
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
deck               0/2     2            0           12h
hook               0/2     2            0           12h
horologium         0/1     1            0           12h
plank              0/1     1            0           12h
sinker             0/1     1            0           12h
statusreconciler   0/1     1            0           12h
tide               0/1     1            0           12h
#+end_example

** TODO ingress

#+begin_src shell
  kubectl get ingress
#+end_src

#+RESULTS:
#+begin_example
NAME   HOSTS   ADDRESS   PORTS   AGE
ing    *                 80      38m
#+end_example

#+begin_src shell
  kubectl get ingress ing -o yaml
#+end_src

#+RESULTS:
#+begin_example
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{},"name":"ing","namespace":"default"},"spec":{"backend":{"serviceName":"deck","servicePort":80},"rules":[{"http":{"paths":[{"backend":{"serviceName":"deck","servicePort":80},"path":"/"},{"backend":{"serviceName":"hook","servicePort":8888},"path":"/hook"}]}}]}}
  creationTimestamp: "2020-05-11T03:11:56Z"
  generation: 1
  name: ing
  namespace: default
  resourceVersion: "1436"
  selfLink: /apis/extensions/v1beta1/namespaces/default/ingresses/ing
  uid: 861040ad-0371-4aa8-9960-e7cb3dd69840
spec:
  backend:
    serviceName: deck
    servicePort: 80
  rules:
  - http:
      paths:
      - backend:
          serviceName: deck
          servicePort: 80
        path: /
      - backend:
          serviceName: hook
          servicePort: 8888
        path: /hook
status:
  loadBalancer: {}
#+end_example

* Rob -> ALB Ingress => other ingress
[[https://aws.amazon.com/blogs/opensource/network-load-balancer-nginx-ingress-controller-eks/][AWS Blog - NLB Nginx Ingress Controller on EKS]]
[[https://kubernetes.github.io/ingress-nginx/deploy/][NGINX Ingress Controller - Install Guide]]
** Network Load Balancer with the NGINX Ingress resource

#+begin_src shell :dir "~/prow-config"
  #  curl -LO https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/aws/deploy.yaml
  # curl -LO https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-0.32.0/deploy/static/provider/aws/deploy.yaml
  kubectl apply -f manifests/ingress/deploy.yaml  # 404s / docs may have moved
  # curl -LO https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml
  curl -LO https://raw.githubusercontent.com/cornellanthony/nlb-nginxIngress-eks/master/nlb-service.yaml
  curl -LO https://raw.githubusercontent.com/cornellanthony/nlb-nginxIngress-eks/master/apple.yaml
  curl -LO  https://raw.githubusercontent.com/cornellanthony/nlb-nginxIngress-eks/master/banana.yaml
#+end_src

#+RESULTS:
#+begin_example
#+end_example

#+begin_src shell
  kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml
  kubectl apply -f https://raw.githubusercontent.com/cornellanthony/nlb-nginxIngress-eks/master/nlb-service.yaml
  kubectl apply -f https://raw.githubusercontent.com/cornellanthony/nlb-nginxIngress-eks/master/apple.yaml
  kubectl apply -f https://raw.githubusercontent.com/cornellanthony/nlb-nginxIngress-eks/master/banana.yaml
#+end_src

** Troubleshooting resources

[[https://eksctl.io/usage/eks-managed-nodes/][EKS Managed Nodes]]
So in AWS Console land in order to grok the nodes you need to
look at EC2 . Do not bother with the EKS Clusters page for reason?

When you logon to the nodes with the unknown state and run the following
#+begin_src shell
[ec2-user@ip-192-168-45-255 ~]$ systemctl status kubelet
● kubelet.service - Kubernetes Kubelet
   Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/kubelet.service.d
           └─10-eksclt.al2.conf
   Active: active (running) since Mon 2020-05-11 03:07:19 UTC; 16h ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 7983 (kubelet)
    Tasks: 83
   Memory: 222.9M
   CGroup: /system.slice/kubelet.service
           ├─ 7983 /usr/bin/kubelet --node-ip=192.168.45.255 --node-labels=role=prow,alpha.eksctl.io/cluster-name=prow-dev,alpha.eksctl.io/nodegroup-name=prow-1,alpha.eksctl.io/instance-id=i-063c273807d19a3...
           └─24396 /usr/bin/python2 -s /usr/bin/aws eks get-token --cluster-name prow-dev --region ap-southeast-2

May 11 19:14:58 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:14:58.711930    7983 reflector.go:125] k8s.io/kubernetes/pkg/kubelet/kubelet.go:445: Failed to list *v1.Se...authorized
May 11 19:14:58 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:14:58.712010    7983 controller.go:125] failed to ensure node lease exists, will retry in 7s, error: Unauthorized
May 11 19:14:58 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:14:58.712078    7983 reflector.go:125] object-"default"/"deck-token-g5pc5": Failed to list *v1.Secret: Unauthorized
May 11 19:14:59 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:14:59.018466    7983 reflector.go:125] object-"kube-system"/"kube-proxy": Failed to list *v1.ConfigMap: Unauthorized
May 11 19:14:59 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:14:59.326603    7983 reflector.go:125] k8s.io/kubernetes/pkg/kubelet/kubelet.go:454: Failed to list *v1.No...authorized
May 11 19:14:59 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:14:59.326665    7983 reflector.go:125] object-"default"/"sinker-token-8pgvp": Failed to list *v1.Secret: Unauthorized
May 11 19:14:59 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:14:59.634835    7983 reflector.go:125] object-"default"/"tide-token-9fqsp": Failed to list *v1.Secret: Unauthorized
May 11 19:14:59 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:14:59.943901    7983 reflector.go:125] object-"default"/"hook-token-dz222": Failed to list *v1.Secret: Unauthorized
May 11 19:14:59 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:14:59.944074    7983 reflector.go:125] object-"default"/"plugins": Failed to list *v1.ConfigMap: Unauthorized
May 11 19:15:00 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:15:00.254296    7983 reflector.go:125] object-"default"/"hmac-token": Failed to list *v1.Secret: Unauthorized
Hint: Some lines were ellipsized, use -l to show in full.
[ec2-user@ip-192-168-45-255 ~]$ date
Mon May 11 19:15:41 UTC 2020
[ec2-user@ip-192-168-45-255 ~]$
#+end_src

#+begin_src shell
 eksctl get --cluster prow-dev nodegroup
#+end_src

#+RESULTS:
#+begin_example
CLUSTER		NODEGROUP	CREATED			MIN SIZE	MAX SIZE	DESIRED CAPACITY	INSTANCE TYPE	IMAGE ID
prow-dev	prow-1		2020-05-11T03:03:02Z	2		4		2			m5d.24xlarge	ami-0bb0c6e35bd291d68
#+end_example
#+begin_src shell
# need to check this
 eksctl delete --cluster prow-dev nodegroup
# pasted result
ii@ip-172-31-4-91:~$ eksctl delete nodegroup --cluster prow-dev prow-1
[ℹ]  eksctl version 0.19.0-rc.1
[ℹ]  using region ap-southeast-2
[ℹ]  combined include rules: prow-1
[ℹ]  1 nodegroup (prow-1) was included (based on the include/exclude rules)
[ℹ]  will delete 1 nodegroups from auth ConfigMap in cluster "prow-dev"
[!]  removing nodegroup from auth ConfigMap: instance identity ARN "arn:aws:iam::928655657136:role/eksctl-prow-dev-nodegroup-prow-1-NodeInstanceRole-1UFBFQ9Q5BFN1" not found in auth ConfigMap
[ℹ]  will drain 1 nodegroup(s) in cluster "prow-dev"
[ℹ]  cordon node "ip-192-168-4-247.ap-southeast-2.compute.internal"
[ℹ]  cordon node "ip-192-168-45-255.ap-southeast-2.compute.internal"
[!]  ignoring DaemonSet-managed Pods: kube-system/aws-node-t9mrd, kube-system/kube-proxy-tggtw
[!]  ignoring DaemonSet-managed Pods: kube-system/aws-node-lc6f5, kube-system/kube-proxy-kxmzh
[!]  ignoring DaemonSet-managed Pods: kube-system/aws-node-t9mrd, kube-system/kube-proxy-tggtw
[!]  ignoring DaemonSet-managed Pods: kube-system/aws-node-lc6f5, kube-system/kube-proxy-kxmzh
[✔]  drained nodes: [ip-192-168-4-247.ap-southeast-2.compute.internal ip-192-168-45-255.ap-southeast-2.compute.internal]
[ℹ]  will delete 1 nodegroups from cluster "prow-dev"
[ℹ]  1 task: { delete nodegroup "prow-1" [async] }
[ℹ]  will delete stack "eksctl-prow-dev-nodegroup-prow-1"
[✔]  deleted 1 nodegroup(s) from cluster "prow-dev"

#+end_src
* Creating a managed nodegroup
[[https://eksctl.io/usage/eks-managed-nodes/][EKS - Creating a cluster]]
#+begin_src shell
eksctl create nodegroup -f eksctl.yaml
#+end_src

#+RESULTS:
#+begin_example
#+end_example

* go get go
#+begin_src shell
  curl -L https://dl.google.com/go/go1.14.2.linux-amd64.tar.gz | sudo tar -C /usr/local -xzf -
#+end_src

#+RESULTS:
#+begin_example
#+end_example

* hook up
Setting up repo with a hook ...
Source coude for the add-hook below.
[[https://github.com/kubernetes/test-infra/blob/dbbeb4216756c3e2bdffa7da6ac0bd97ead001e4/experiment/add-hook/main.go][hook main.go]]

Bazel separates flags ro the command being run using --
Here for example, bazel refuses to parse --help (no wonder nobody understands it!) so in order to have --help interpred by the add-hook code prepend -- first
~
ii@ip-172-31-4-91 ~/test-infra $ bazel run //experiment/add-hook -- --help
INFO: Analyzed target //experiment/add-hook:add-hook (1 packages loaded, 556 targets configured).
INFO: Found 1 target...
INFO: From Generating Descriptor Set proto_library @go_googleapis//google/iam/v1:iam_proto:
google/iam/v1/options.proto:20:1: warning: Import google/api/annotations.proto is unused.
google/iam/v1/policy.proto:21:1: warning: Import google/api/annotations.proto is unused.
Target //experiment/add-hook:add-hook up-to-date:
  bazel-bin/experiment/add-hook/linux_amd64_stripped/add-hook
INFO: Elapsed time: 76.356s, Critical Path: 17.69s
INFO: 213 processes: 213 linux-sandbox.
INFO: Build completed successfully, 215 total actions
INFO: Build completed successfully, 215 total actions
Usage of /newhome/ii/.cache/bazel/_bazel_ii/8dad4840a73c734f8c8c7e2d452a8/execroot/io_k8s_test_infra/bazel-out/k8-fastbuild/bin/experiment/add-hook/linux_amd64_stripped/add-hook:
  -confirm
        Apply changes to github
  -event value
        Receive hooks for the following events, defaults to ["*"] (all events) (default *)
  -github-endpoint value
        GitHub's API endpoint (may differ for enterprise). (default https://api.github.com)
  -github-graphql-endpoint string
        GitHub GraphQL API endpoint (may differ for enterprise). (default "https://api.github.com/graphql")
  -github-host string
        GitHub's default host (may differ for enterprise) (default "github.com")
  -github-token-path string
        Path to the file containing the GitHub OAuth secret.
  -hmac-path string
        Path to hmac secret
  -hook-url string
        URL to send hooks
  -repo value
        Add hooks for this org or org/repo
~

#+begin_src shell :prologue "export PATH=/usr/local/go/bin:$PATH\n"
  echo $PATH
  go get -u k8s.io/test-infra/experiment/add-hook
  add-hook
#+end_src

#+RESULTS:
#+begin_example
#+end_example

#+begin_src shell :prologue "export PATH=/usr/local/go/bin:$PATH\n"
  add-hook
#+end_src

#+RESULTS:
#+begin_example
#+end_example

#+begin_src shell :dir "~/test-infra"
  (
  bazel run //experiment/add-hook -- \
    --github-endpoint=http://ghproxy/
    --github-token-path=../prow-config/.secret-oauth \
    --hmac-path=../prow-config/.secret-hook \
    --hook-url http://prow.cncf.io/hook \
    --repo cncf/k8s-conformance \
    --repo cncf/apisnoop \
    --repo cncf-infra/prow-config \
  ) 2>&1
# --confirm=false  # Remove =false to actually add hook
  :
#+end_src

#+RESULTS:
#+begin_example
Starting local Bazel server and connecting to it...
Loading:
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
    currently loading: experiment/add-hook
Analyzing: target //experiment/add-hook:add-hook (1 packages loaded, 0 targets configured)
Analyzing: target //experiment/add-hook:add-hook (31 packages loaded, 5708 targets configured)
Analyzing: target //experiment/add-hook:add-hook (71 packages loaded, 6593 targets configured)
Analyzing: target //experiment/add-hook:add-hook (225 packages loaded, 7260 targets configured)
Analyzing: target //experiment/add-hook:add-hook (390 packages loaded, 8922 targets configured)
Analyzing: target //experiment/add-hook:add-hook (597 packages loaded, 10308 targets configured)
INFO: Analyzed target //experiment/add-hook:add-hook (597 packages loaded, 10309 targets configured).
INFO: Found 1 target...
INFO: Deleting stale sandbox base /newhome/ii/.cache/bazel/_bazel_ii/8dad4840a73c734ffda8c8c7e2d452a8/sandbox
[0 / 29] [Prepa] BazelWorkspaceStatusAction stable-status.txt
[24 / 765] Compiling external/com_google_protobuf/src/google/protobuf/any_lite.cc [for host]; 0s linux-sandbox ... (8 actions, 7 running)
[42 / 765] Compiling external/com_google_protobuf/src/google/protobuf/extension_set.cc [for host]; 1s linux-sandbox ... (8 actions, 7 running)
[54 / 765] Compiling external/com_google_protobuf/src/google/protobuf/extension_set.cc [for host]; 3s linux-sandbox ... (8 actions, 7 running)
[71 / 765] Compiling external/com_google_protobuf/src/google/protobuf/generated_message_table_driven_lite.cc [for host]; 4s linux-sandbox ... (8 actions, 7 running)
[79 / 765] Compiling external/com_google_protobuf/src/google/protobuf/text_format.cc [for host]; 3s linux-sandbox ... (8 actions, 7 running)
[88 / 765] Compiling external/com_google_protobuf/src/google/protobuf/generated_message_reflection.cc [for host]; 2s linux-sandbox ... (8 actions, 7 running)
[104 / 765] Compiling external/com_google_protobuf/src/google/protobuf/map_field.cc [for host]; 3s linux-sandbox ... (8 actions, 7 running)
[123 / 765] Compiling external/com_google_protobuf/src/google/protobuf/wire_format.cc [for host]; 4s linux-sandbox ... (7 actions, 6 running)
[134 / 765] Compiling external/com_google_protobuf/src/google/protobuf/descriptor.cc [for host]; 6s linux-sandbox ... (8 actions running)
[144 / 765] Compiling external/com_google_protobuf/src/google/protobuf/descriptor.cc [for host]; 11s linux-sandbox ... (8 actions running)
[157 / 765] Compiling external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_file.cc [for host]; 5s linux-sandbox ... (8 actions running)
[175 / 765] Compiling external/com_google_protobuf/src/google/protobuf/struct.pb.cc [for host]; 4s linux-sandbox ... (8 actions, 7 running)
[194 / 765] Compiling external/com_google_protobuf/src/google/protobuf/compiler/java/java_primitive_field_lite.cc [for host]; 2s linux-sandbox ... (8 actions, 7 running)
[243 / 953] Compiling external/com_google_protobuf/src/google/protobuf/compiler/command_line_interface.cc [for host]; 3s linux-sandbox ... (8 actions, 7 running)
[272 / 953] Compiling external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_message.cc; 8s linux-sandbox ... (8 actions, 7 running)
[305 / 953] Compiling external/com_google_protobuf/src/google/protobuf/compiler/java/java_primitive_field.cc; 2s linux-sandbox ... (8 actions, 7 running)
[340 / 953] Compiling external/com_google_protobuf/src/google/protobuf/compiler/python/python_generator.cc; 4s linux-sandbox ... (8 actions, 7 running)
[389 / 953] Compiling external/com_google_protobuf/src/google/protobuf/wire_format.cc; 3s linux-sandbox ... (8 actions, 7 running)
INFO: From Generating Descriptor Set proto_library @go_googleapis//google/iam/v1:iam_proto:
google/iam/v1/options.proto:20:1: warning: Import google/api/annotations.proto is unused.
google/iam/v1/policy.proto:21:1: warning: Import google/api/annotations.proto is unused.
Target //experiment/add-hook:add-hook up-to-date:
  bazel-bin/experiment/add-hook/linux_amd64_stripped/add-hook
INFO: Elapsed time: 121.350s, Critical Path: 15.54s
INFO: 373 processes: 373 linux-sandbox.
INFO: Build completed successfully, 376 total actions
INFO: Running command line: bazel-bin/experiment/add-hook/linux_amd64_stripped/add-hook '--hmac-path=../prow-config/.secret-hook' '--github-token-path=../prow-config/.secret-oauth' --hook-url http://prow.cncf.io/hook --repo cncf/k8s-conformance --repo cncf/apisnoop
INFO: Build completed successfully, 376 total actions
time="2020-05-18T18:54:07Z" level=warning msg="It doesn't look like you are using ghproxy to cache API calls to GitHub! This has become a required component of Prow and other components will soon be allowed to add features that may rapidly consume API ratelimit without caching. Starting May 1, 2020 use Prow components without ghproxy at your own risk! https://github.com/kubernetes/test-infra/tree/master/ghproxy#ghproxy"
time="2020-05-18T18:54:07Z" level=fatal msg="Could not create github client: start ../prow-config/.secret-oauth: error reading ../prow-config/.secret-oauth: open ../prow-config/.secret-oauth: no such file or directory"
/bin/bash: line 9: --confirm=false: command not found
#+end_example
* Adding more repos to prow
- The new repo will need to be defined in the hook above, but also added to plugins
** content of plugins.yaml showing cncf/k8s-conformance added
#+begin_src  shell
  cat plugins.yaml
#+end_src

#+RESULTS:
#+begin_example
# plugin-specific config

# config-updater
# update prow cluster's configmaps from the repo with this plugin enabled; assumed to be a single repo
config_updater:
  maps:
    config.yaml:
      name: config
    plugins.yaml:
      name: plugins
    jobs/**/*.yaml:
      name: job-config

# which plugins should be enabled for which orgs or org/repos
plugins:
  cncf-infra:
  # - approve
  - assign
  - cat
  - dog
  - hold
  - label
  - lgtm
  # - owners-label
  - pony
  - shrug
  - size
  - skip
  - trigger
  - wip
  # - verify-owners
  - yuks

  cncf-infra/prow-config:
  - config-updater

  cncf/k8s-conformance:
  # - approve
  - assign
  - cat
  - dog
  - hold
  - label
  - lgtm
  # - owners-label
  - pony
  - shrug
  - size
  - skip
  - trigger
  - wip
  # - verify-owners
  - yuks
#+end_example

- After updating plugins run the following to apply it it the cluster.
** Lets apply the change
#+begin_src  shell
  kubectl create configmap plugins --from-file=plugins.yaml=./plugins.yaml  --dry-run -o yaml | kubectl replace configmap plugins -f -
#+end_src

#+RESULTS:
#+begin_example
configmap/plugins replaced
#+end_example

* ghproxy

#+begin_src shell
  kubectl apply -f manifests/ghproxy.yaml
#+end_src

#+RESULTS:
#+begin_example
persistentvolumeclaim/ghproxy created
deployment.apps/ghproxy created
service/ghproxy created
#+end_example
* Verifying Conformance Certification Requests
Live Repo : https://github.com/cncf/k8s-conformance
Test Repo : https://github.com/cncf-infra/k8s-conformance a fork of the cncf repo

https://github.com/cncf/apisnoop/projects/29
https://github.com/cncf/apisnoop/issues/342
** Requirements
Check the consistencey of the PR to the above repos
Ensure that the versoin referenced in the PR Title corresponds to the version of k8s referenced in the supplied logs

** Design
Implement as a [[https://github.com/kubernetes/test-infra/tree/master/prow/plugins#external-plugins][External Plugin]] that interacts but is no linked into the Hook component of Prow

** Implementation
*** Plugin
name verify-conformance-request
desc Checks a k8s-conformance PR to see if it is internally consitent.
*** Development setup
Code location
/home/ii/go/src/k8s.io/test-infra/prow/external-plugins/verify-conformance-request
*** Building Code

In the mean time following the steps below
** Literate Build of the go code
Execute the block below using ,,
So note here that we are bulding locally on the host
And developing the plugin in the k8s/test-infra clone while we figure out how to vendor k8s/test-infra/prow dependancies.

#+BEGIN_SRC shell
# Workaround for above is to place the cncf plugin in to the the k8s/infra code base
cd ~/go/src/k8s.io/test-infra/prow/external-plugins/verify-conformance-request
# make changes
go build
cp verify-conformance-request /home/ii/prow-config/prow/external-plugins/verify-conformance-request/
ls -al /home/ii/prow-config/prow/external-plugins/verify-conformance-request/
#+END_SRC

#+RESULTS:
#+begin_example
total 51412
drwxrwxr-x 4 ii ii     4096 Jun 10 21:50 .
drwxrwxr-x 3 ii ii     4096 May 26 16:34 ..
-rw-rw-r-- 1 ii ii       40 May 26 17:02 .secret-hook
-rw-rw-r-- 1 ii ii       40 May 26 17:02 .secret-oauth
-rw-rw-r-- 1 ii ii      807 Jun 10 21:50 Dockerfile
-rw-rw-r-- 1 ii ii     5603 May 26 18:33 main.go
drwxrwxr-x 2 ii ii     4096 May 24 17:38 plugin
drwxrwxr-x 2 ii ii     4096 Jun  3 19:19 test-data
-rw-rw-r-- 1 ii ii      140 Jun  6 11:05 vcr.yaml
-rwxrwxr-x 1 ii ii 52603642 Jun 11 18:02 verify-conformance-request
#+end_example

*** Running the external plugin locally

#+BEGIN_SRC lang=shell
$ ./verify-conformance-request --hmac-secret-file=/home/ii/.secret-hook --github-token-path=/home/ii/.secret-oauth --plugin-config=/home/ii/prow-config/plugins.yaml
#+END_SRC

  *** Building Container
  [[https://github.com/kubernetes/test-infra/blob/master/prow/build_test_update.md#how-to-test-a-plugin][How to test a plugin]]
  Test data has been placeid in /home/ii/prow-config/prow/external-plugins/verify-conformance-request/test-data/open-pr.json
  You can send a test webhook using phony as follows:
#+BEGIN_SRC shell
 bazel run //prow/cmd/phony -- \
  --address=http://localhost:8888/hook \
  --hmac="secret_text_does_here" --event=pull_request \
  --payload=/home/ii/prow-config/prow/external-plugins/verify-conformance-request/test-data/open-pr.json
#+END_SRC
N.B. the ~--hmac~ flag requires a string with the text of the hmacs secret.

* Build and push the container
** Make sure that the Building code step above is done and that you have the binary copied into the prow-config repo
** build the container and tag
- Will build this as a container and publish to the cncf-infra ECR repository [[https://console.aws.amazon.com/ecr/repositories/cncf-infra/?region=us-east-1][ecr/repo cncf-infra]]
- The link above will also provide you with a list of commands to run if you get stuck
- TODO: Bryan is updating repo to be the plugin name instead of cncf-prow, once it is up change this push to go to verify-conformance-request (rememver to update verify-conformance-deploy.yaml

Remember to change the version
Bryan is doing work on this so this will as he rolls out new procedures. Thanks Bryan!
  #+BEGIN_SRC bash
    cd /home/ii/prow-config/prow/external-plugins/verify-conformance-request
    aws ecr get-login-password --region ap-southeast-2  | docker login --username AWS --password-stdin 928655657136.dkr.ecr.ap-southeast-2.amazonaws.com
    docker build -t cncf-prow .
    docker tag cncf-prow:latest 928655657136.dkr.ecr.ap-southeast-2.amazonaws.com/cncf-prow:v1.3
    docker push 928655657136.dkr.ecr.ap-southeast-2.amazonaws.com/cncf-prow:v1.3
  #+END_SRC

  #+RESULTS:
  #+begin_src bash
  Login Succeeded
  The push refers to repository [928655657136.dkr.ecr.ap-southeast-2.amazonaws.com/cncf-prow]
  e8dcdc465683: Preparing
  c52a2bd34fd7: Preparing
  5a92b57a5aac: Preparing
  5835b741b51f: Preparing
  02c991ab8711: Preparing
  bee1c15bf7e8: Preparing
  423d63eb4a27: Preparing
  7f9bf938b053: Preparing
  f2b4f0674ba3: Preparing
  7f9bf938b053: Waiting
  423d63eb4a27: Waiting
  f2b4f0674ba3: Waiting
  bee1c15bf7e8: Waiting
  5835b741b51f: Layer already exists
  c52a2bd34fd7: Layer already exists
  02c991ab8711: Layer already exists
  e8dcdc465683: Layer already exists
  5a92b57a5aac: Layer already exists
  bee1c15bf7e8: Layer already exists
  7f9bf938b053: Layer already exists
  f2b4f0674ba3: Layer already exists
  423d63eb4a27: Layer already exists
  v1.3: digest: sha256:ed6402db6f8458e08bd6b9cf334d99fbdfc749d69473f006069010ccca149fc7 size: 2214
  #+end_src


** Run the container to make sure it is working (optional step that can be used for troubleshooting)
#+begin_src bash
  docker run  -v /home/ii/.secret-hook:/etc/webhook/hmac -v /home/ii/.secret-oauth:/etc/github/oauth -v /home/ii/prow-config/prow/external-plugins/verify-conformance-request/vcr.yaml:/plugin/vcr.yaml -v /home/ii/prow-config/plugins.yaml:/etc/plugins/plugins.yaml 847cf1d2cf02  /bin/bash -c "/plugin/verify-conformance-request --hmac-secret-file=/etc/webhook/hmac --github-token-path=/etc/github/oauth --plugin-config=/plugin/vcr.yaml --update-period=1m"
#+end_src

  #+RESULTS:
  #+begin_src bash
time="2020-06-10T07:03:37Z" level=warning msg="It doesn't look like you are using ghproxy to cache API calls to GitHub! This has become a required component of Prow and other components will soon be allowed to add features that may rapidly consume API ratelimit without caching. Starting May 1, 2020 use Prow components without ghproxy at your own risk! https://github.com/kubernetes/test-infra/tree/master/ghproxy#ghproxy"
time="2020-06-10T07:03:37Z" level=warning msg="missing required flag: please set to --github-token-path=/etc/github/oauth before June 2020"
time="2020-06-10T07:03:37Z" level=info msg="Throttle(360, 360)" client=github
time="2020-06-10T07:03:37Z" level=info msg="verify-conformance-request : HandleAll : Checking all PRs for handling" plugin=verify-conformance-request
time="2020-06-10T07:03:37Z" level=warning msg="HandleAll : No repos have been configured for the verify-conformance-request plugin" plugin=verify-conformance-request
time="2020-06-10T07:03:37Z" level=info msg="Periodic update complete." duration="89.344µs" plugin=verify-conformance-request
#+end_src

*** I do not understand why the above docker run is not seeing the repo
    - I did notice if I exec into that container and run the command in -c it works as expected
#+begin_src bash
docker exec -i -t f39470700e75 bash
root@f39470700e75:/plugin# cat /plugin/vcr.yaml
external_plugins:
  cncf-infra/k8s-conformance:
  - name: verify-conformance-request
    events:
      - issue_comment
     - pull_request
root@f39470700e75:/plugin# /plugin/verify-conformance-request --hmac-secret-file=/etc/webhook/hmac --github-token-path=/etc/github/oauth --plugin-config=/plugin/vcr.yaml --update-period=1m
WARN[0000] It doesn't look like you are using ghproxy to cache API calls to GitHub! This has become a required component of Prow and other components will soon be allowed to add features that may rapidly consume API ratelimit without caching. Starting May 1, 2020 use Prow components without ghproxy at your own risk! https://github.com/kubernetes/test-infra/tree/master/ghproxy#ghproxy
WARN[0000] no plugins specified-- check syntax?
INFO[0000] Throttle(360, 360)                            client=github
INFO[0000] verify-conformance-request : HandleAll : Checking all PRs for handling  plugin=verify-conformance-request
INFO[0000] Server exited.                                error="listen tcp :8888: bind: address already in use"
INFO[0000] Search for query "archived:false is:pr is:open repo:"cncf-infra/k8s-conformance"" cost 1 point(s). 4991 remaining.  plugin=verify-conformance-request
INFO[0000] Considering 1 PRs.                            plugin=verify-conformance-request
INFO[0000] IsVerifiable: title of PR is "NOT A REAL CONFORMANCE REQ for  v1.18"  plugin=verify-conformance-request
INFO[0000] AddLabel(cncf-infra, k8s-conformance, 1, verifiable)  client=github
#+end_src

    - For now I am going to call this good and use the above flags to build out the verify-conformance-deploy.yaml

* Next steps update verify-conformance-deployment.yaml to emulate the docker run
Also, ensure that you are referencing the tag of the image that you just built
** loading config map for vcr.yaml
 #+begin_src shell
   kubectl delete configmap vcr-config
   kubectl create configmap vcr-config --from-file=/home/ii/prow-config/prow/external-plugins/verify-conformance-request/vcr.yaml
 #+end_src

 #+RESULTS:
 configmap/vcr-config created
 #+end_example

** apply verify-conformance-deployment.yaml
#+begin_src shell :dir "~/prow-config"
  kubectl apply -f manifests/verify-conformance-deployment.yaml
#+end_src

#+RESULTS:
#+begin_example
deployment.apps/verify-conformance-request configured
#+end_example

*** Lets look at the pods.
#+begin_src shell
  kubectl get pods
#+end_src

#+RESULTS:
#+begin_example
NAME                                          READY   STATUS    RESTARTS   AGE
deck-7c6d46b4f7-dp68c                         1/1     Running   0          30d
deck-7c6d46b4f7-n8x5q                         1/1     Running   0          30d
ghproxy-75ddf48577-g9bbs                      1/1     Running   0          30d
hook-59bb5f886d-24sw2                         1/1     Running   0          23d
hook-59bb5f886d-d29vb                         1/1     Running   0          23d
horologium-54f95c4dc4-z58sb                   1/1     Running   0          30d
plank-7cf6bf5cb6-979ph                        1/1     Running   0          30d
sinker-ddf8cbcb6-8tl7f                        1/1     Running   1          30d
statusreconciler-b946855cf-l5x7g              1/1     Running   0          30d
tide-66b57f5ccf-x6j67                         1/1     Running   0          30d
verify-conformance-request-5798fc7694-sbjn8   1/1     Running   0          15s
#+end_example

*** Initially we crashed without logs, this helped me get a meaningful error
#+begin_src shell
kubectl describe pod verify-conformance-request-5b7647499f-lr49f
#+end_src

#+RESULTS:
#+begin_example
Name:           verify-conformance-request-5b7647499f-lr49f
Namespace:      default
Priority:       0
Node:           ip-192-168-18-143.ap-southeast-2.compute.internal/192.168.18.143
Start Time:     Thu, 11 Jun 2020 04:18:30 +0000
Labels:         app=verify-conformance-request
                pod-template-hash=5b7647499f
Annotations:    kubernetes.io/psp: eks.privileged
Status:         Running
IP:             192.168.31.154
IPs:            <none>
Controlled By:  ReplicaSet/verify-conformance-request-5b7647499f
Containers:
  verify-conformance-request:
    Container ID:  docker://2b3be86482ef64fae7981029d847534e8f6e2197b0812db0df6229ecf4bef58c
    Image:         928655657136.dkr.ecr.ap-southeast-2.amazonaws.com/cncf-prow:v1.2
    Image ID:      docker-pullable://928655657136.dkr.ecr.ap-southeast-2.amazonaws.com/cncf-prow@sha256:7374eaa38823120333d2ea8b31ceef0110e0dc6be34f775ae2054858ddee2da5
    Port:          8888/TCP
    Host Port:     0/TCP
    Args:
      --dry-run=false
      --github-endpoint=http://ghproxy
      --github-endpoint=https://api.github.com
      --hmac-secret-file=/etc/webhook/hmac
      --github-token-path=/etc/github/oauth
      --plugin-config=/plugin/vcr.yaml
      --update-period=1m
    State:          Running
      Started:      Thu, 11 Jun 2020 04:18:32 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/github from oauth (ro)
      /etc/plugins from plugins (ro)
      /etc/webhook from hmac (ro)
      /plugin/vcr.yaml from vcr-config (ro,path="vcr.yaml")
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-lsxfc (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  hmac:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  hmac-token
    Optional:    false
  oauth:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  oauth-token
    Optional:    false
  plugins:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      plugins
    Optional:  false
  vcr-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      vcr-config
    Optional:  false
  default-token-lsxfc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-lsxfc
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>
#+end_example

*** See if the logs will tell us anything.
#+begin_src shell
  kubectl logs verify-conformance-request-5b7647499f-lr49f | tail -20
#+end_src

#+RESULTS:
#+begin_example
#+end_example


*  Random header to stop the content below accidentally getting collapsed with another header

  This is how [[https://github.com/kubernetes/test-infra/blob/100609e548a3cca9f007557727ed83dee0992b14/config/prow/cluster/needs-rebase_deployment.yaml][test-infra deploy needs-rebase external plugin]]

  *** Configuration
  #+BEGIN_SRC lang=yaml
  plugins:
    cncf-infra/k8s-conformance:
    # - approve
    - verify-conf-request
    - assign
  #+END_SRC





  * Footnotes
  ** software
  *** direnv
  *** aws-iam-authenticator
  https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html
  ** gotchas
  *** documentation seems to call it the oauth secret.... when in fact it's a github personal access tokens
  *** cluster authentication / iam
  https://github.com/kubernetes-sigs/aws-iam-authenticator/issues/174#issuecomment-450651720

  *** cluster-admin role
  #+BEGIN_SRC sh
    kubectl get clusterrolebinding cluster-admin -o yaml
  #+END_SRC

  #+RESULTS:
  #+begin_src sh
  apiVersion: rbac.authorization.k8s.io/v1verify-conformance-deployment.yaml
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: "2020-04-06T04:19:41Z"
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: cluster-admin
    resourceVersion: "95"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/cluster-admin
    uid: c8c1eb3a-72a4-45d3-8ae2-c7d8abda71ee
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: cluster-admin
  subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: system:masters
  #+end_src
  ** ENV for aws cli
  https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html

  **AWS_PROFILE**
