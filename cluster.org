* BOARD
** [[https://github.com/cncf/apisnoop/projects/29#column-8865828][prow.cncf.io-TODO]]
*** TODO ghproxy
*** TODO plank
    [[https://github.com/kubernetes/test-infra/tree/0bde37ecf717799c2953131192c997373ffe976d/prow/cmd/plank][Plank]]
    [[https://github.com/kubernetes/test-infra/blob/0bde37ecf717799c2953131192c997373ffe976d/prow/pod-utilities.md][Pod Utilities]]
*** DONE tide
    CLOSED: [2020-05-14 Thu 03:21]
    -  https://prow.k8s.io/tide
    So here we see Merge Requirements and repos that have incoming PRs
    - cf. http://prow.cncf.io/tide
    - [[https://github.com/kubernetes/test-infra/blob/0bde37ecf717799c2953131192c997373ffe976d/config/prow/config.yaml#L351][Tide Config for test-infra]]

** [[https://github.com/cncf/apisnoop/projects/29#column-8865858][cncf/k8s-conformance-TODO]]
*** TODO confirm version of k8s being tested matched foldera
*** TODO generate list of tests required for a specific version
*** TODO generate list of tests submitted as run by a PR
*** TODO add blocking tag if any requeired tests not run
*** TODO comment with list of missing tests
* strat
** get presubmits job results showing up as comments
** optional
** required
** simple presubmit job (cat/dog) counts something [[https://github.com/cncf/k8s-conformance][cncf/k8s-conformance]]

* Setup a cluster

Used *eksctl* though long term, maybe we use cluster-api and [[https://github.com/aws/aws-service-operator-k8s/blob/master/docs/background.md#custom-controllers-and-operators-in-aws][aws-service-operation-k8s]]

#+begin_src  shell
aws eks list-clusters
#+end_src

#+RESULTS:
#+begin_example
{
    "clusters": [
        "prow-dev"
    ]
}
#+end_example
* loading secrets
** github-hmac / hook
 #+begin_src shell
   kubectl delete secret hmac-token
   kubectl create secret generic hmac-token --from-file=hmac=.secret-hook
 #+end_src

 #+RESULTS:
 #+begin_example
 secret/hmac-token created
 #+end_example

** github-oauth
 #+begin_src shell
   kubectl delete secret oauth-token
   kubectl create secret generic oauth-token --from-file=oauth=.secret-oauth
 #+end_src

 #+RESULTS:
 #+begin_example
 secret "oauth-token" deleted
 secret/oauth-token created
 #+end_example

* prow components manifst
** cluster/starter.yaml
https://github.com/kubernetes/test-infra/blob/master/prow/getting_started_deploy.md#add-the-prow-components-to-the-cluster
#+begin_src shell :dir "~/prow-config"
  kubectl apply -f manifests/starter.yaml
#+end_src

#+RESULTS:
#+begin_example
configmap/plugins created
configmap/config created
customresourcedefinition.apiextensions.k8s.io/prowjobs.prow.k8s.io created
deployment.apps/hook created
service/hook created
deployment.apps/plank created
deployment.apps/sinker created
deployment.apps/deck created
service/deck created
deployment.apps/horologium created
deployment.apps/tide created
service/tide created
ingress.extensions/ing created
deployment.apps/statusreconciler created
namespace/test-pods created
serviceaccount/deck created
rolebinding.rbac.authorization.k8s.io/deck created
rolebinding.rbac.authorization.k8s.io/deck created
role.rbac.authorization.k8s.io/deck created
role.rbac.authorization.k8s.io/deck created
serviceaccount/horologium created
role.rbac.authorization.k8s.io/horologium created
rolebinding.rbac.authorization.k8s.io/horologium created
serviceaccount/plank created
role.rbac.authorization.k8s.io/plank created
role.rbac.authorization.k8s.io/plank created
rolebinding.rbac.authorization.k8s.io/plank created
rolebinding.rbac.authorization.k8s.io/plank created
serviceaccount/sinker created
role.rbac.authorization.k8s.io/sinker created
role.rbac.authorization.k8s.io/sinker created
rolebinding.rbac.authorization.k8s.io/sinker created
rolebinding.rbac.authorization.k8s.io/sinker created
serviceaccount/hook created
role.rbac.authorization.k8s.io/hook created
rolebinding.rbac.authorization.k8s.io/hook created
serviceaccount/tide created
role.rbac.authorization.k8s.io/tide created
rolebinding.rbac.authorization.k8s.io/tide created
serviceaccount/statusreconciler created
role.rbac.authorization.k8s.io/statusreconciler created
rolebinding.rbac.authorization.k8s.io/statusreconciler created
#+end_example

* components
** services
#+begin_src shell
  kubectl get services
#+end_src

#+RESULTS:
#+begin_example
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
deck         NodePort    10.100.33.217    <none>        80:32096/TCP     12h
hook         NodePort    10.100.156.63    <none>        8888:31025/TCP   12h
kubernetes   ClusterIP   10.100.0.1       <none>        443/TCP          13h
tide         NodePort    10.100.132.246   <none>        80:31081/TCP     12h
#+end_example

** pods
#+begin_src shell
  kubectl get pods
#+end_src

#+RESULTS:
#+begin_example
NAME                               READY   STATUS    RESTARTS   AGE
deck-7c6d46b4f7-8nj26              1/1     Running   0          12h
deck-7c6d46b4f7-p7lws              1/1     Running   0          12h
hook-5f4db6758f-g7dzs              1/1     Running   0          12h
hook-5f4db6758f-rcg87              1/1     Running   0          12h
horologium-54f95c4dc4-5km7m        1/1     Running   0          12h
plank-7cf6bf5cb6-9njpm             1/1     Running   0          12h
sinker-ddf8cbcb6-fxbzb             1/1     Running   0          12h
statusreconciler-b946855cf-jt89w   1/1     Running   0          12h
tide-66b57f5ccf-wsns5              1/1     Running   0          12h
#+end_example

** deployment

#+begin_src shell
  kubectl get deployments
#+end_src

#+RESULTS:
#+begin_example
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
deck               0/2     2            0           12h
hook               0/2     2            0           12h
horologium         0/1     1            0           12h
plank              0/1     1            0           12h
sinker             0/1     1            0           12h
statusreconciler   0/1     1            0           12h
tide               0/1     1            0           12h
#+end_example

** TODO ingress

#+begin_src shell
  kubectl get ingress
#+end_src

#+RESULTS:
#+begin_example
NAME   HOSTS   ADDRESS   PORTS   AGE
ing    *                 80      38m
#+end_example

#+begin_src shell
  kubectl get ingress ing -o yaml
#+end_src

#+RESULTS:
#+begin_example
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{},"name":"ing","namespace":"default"},"spec":{"backend":{"serviceName":"deck","servicePort":80},"rules":[{"http":{"paths":[{"backend":{"serviceName":"deck","servicePort":80},"path":"/"},{"backend":{"serviceName":"hook","servicePort":8888},"path":"/hook"}]}}]}}
  creationTimestamp: "2020-05-11T03:11:56Z"
  generation: 1
  name: ing
  namespace: default
  resourceVersion: "1436"
  selfLink: /apis/extensions/v1beta1/namespaces/default/ingresses/ing
  uid: 861040ad-0371-4aa8-9960-e7cb3dd69840
spec:
  backend:
    serviceName: deck
    servicePort: 80
  rules:
  - http:
      paths:
      - backend:
          serviceName: deck
          servicePort: 80
        path: /
      - backend:
          serviceName: hook
          servicePort: 8888
        path: /hook
status:
  loadBalancer: {}
#+end_example

* Rob -> ALB Ingress => other ingress
[[https://aws.amazon.com/blogs/opensource/network-load-balancer-nginx-ingress-controller-eks/][AWS Blog - NLB Nginx Ingress Controller on EKS]]
[[https://kubernetes.github.io/ingress-nginx/deploy/][NGINX Ingress Controller - Install Guide]]
** Network Load Balancer with the NGINX Ingress resource

#+begin_src shell :dir "~/prow-config"
  #  curl -LO https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/aws/deploy.yaml
  # curl -LO https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-0.32.0/deploy/static/provider/aws/deploy.yaml
  kubectl apply -f manifests/ingress/deploy.yaml  # 404s / docs may have moved
  # curl -LO https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml
  curl -LO https://raw.githubusercontent.com/cornellanthony/nlb-nginxIngress-eks/master/nlb-service.yaml
  curl -LO https://raw.githubusercontent.com/cornellanthony/nlb-nginxIngress-eks/master/apple.yaml
  curl -LO  https://raw.githubusercontent.com/cornellanthony/nlb-nginxIngress-eks/master/banana.yaml
#+end_src

#+RESULTS:
#+begin_example
#+end_example

#+begin_src shell
  kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml
  kubectl apply -f https://raw.githubusercontent.com/cornellanthony/nlb-nginxIngress-eks/master/nlb-service.yaml
  kubectl apply -f https://raw.githubusercontent.com/cornellanthony/nlb-nginxIngress-eks/master/apple.yaml
  kubectl apply -f https://raw.githubusercontent.com/cornellanthony/nlb-nginxIngress-eks/master/banana.yaml
#+end_src

** Troubleshooting resources

[[https://eksctl.io/usage/eks-managed-nodes/][EKS Managed Nodes]]
So in AWS Console land in order to grok the nodes you need to
look at EC2 . Do not bother with the EKS Clusters page for reason?

When you logon to the nodes with the unknown state and run the following
#+begin_src shell
[ec2-user@ip-192-168-45-255 ~]$ systemctl status kubelet
● kubelet.service - Kubernetes Kubelet
   Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/kubelet.service.d
           └─10-eksclt.al2.conf
   Active: active (running) since Mon 2020-05-11 03:07:19 UTC; 16h ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 7983 (kubelet)
    Tasks: 83
   Memory: 222.9M
   CGroup: /system.slice/kubelet.service
           ├─ 7983 /usr/bin/kubelet --node-ip=192.168.45.255 --node-labels=role=prow,alpha.eksctl.io/cluster-name=prow-dev,alpha.eksctl.io/nodegroup-name=prow-1,alpha.eksctl.io/instance-id=i-063c273807d19a3...
           └─24396 /usr/bin/python2 -s /usr/bin/aws eks get-token --cluster-name prow-dev --region ap-southeast-2

May 11 19:14:58 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:14:58.711930    7983 reflector.go:125] k8s.io/kubernetes/pkg/kubelet/kubelet.go:445: Failed to list *v1.Se...authorized
May 11 19:14:58 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:14:58.712010    7983 controller.go:125] failed to ensure node lease exists, will retry in 7s, error: Unauthorized
May 11 19:14:58 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:14:58.712078    7983 reflector.go:125] object-"default"/"deck-token-g5pc5": Failed to list *v1.Secret: Unauthorized
May 11 19:14:59 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:14:59.018466    7983 reflector.go:125] object-"kube-system"/"kube-proxy": Failed to list *v1.ConfigMap: Unauthorized
May 11 19:14:59 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:14:59.326603    7983 reflector.go:125] k8s.io/kubernetes/pkg/kubelet/kubelet.go:454: Failed to list *v1.No...authorized
May 11 19:14:59 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:14:59.326665    7983 reflector.go:125] object-"default"/"sinker-token-8pgvp": Failed to list *v1.Secret: Unauthorized
May 11 19:14:59 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:14:59.634835    7983 reflector.go:125] object-"default"/"tide-token-9fqsp": Failed to list *v1.Secret: Unauthorized
May 11 19:14:59 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:14:59.943901    7983 reflector.go:125] object-"default"/"hook-token-dz222": Failed to list *v1.Secret: Unauthorized
May 11 19:14:59 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:14:59.944074    7983 reflector.go:125] object-"default"/"plugins": Failed to list *v1.ConfigMap: Unauthorized
May 11 19:15:00 ip-192-168-45-255.ap-southeast-2.compute.internal kubelet[7983]: E0511 19:15:00.254296    7983 reflector.go:125] object-"default"/"hmac-token": Failed to list *v1.Secret: Unauthorized
Hint: Some lines were ellipsized, use -l to show in full.
[ec2-user@ip-192-168-45-255 ~]$ date
Mon May 11 19:15:41 UTC 2020
[ec2-user@ip-192-168-45-255 ~]$
#+end_src

#+begin_src shell
 eksctl get --cluster prow-dev nodegroup
#+end_src

#+RESULTS:
#+begin_example
CLUSTER		NODEGROUP	CREATED			MIN SIZE	MAX SIZE	DESIRED CAPACITY	INSTANCE TYPE	IMAGE ID
prow-dev	prow-1		2020-05-11T03:03:02Z	2		4		2			m5d.24xlarge	ami-0bb0c6e35bd291d68
#+end_example
#+begin_src shell
# need to check this
 eksctl delete --cluster prow-dev nodegroup
# pasted result
ii@ip-172-31-4-91:~$ eksctl delete nodegroup --cluster prow-dev prow-1
[ℹ]  eksctl version 0.19.0-rc.1
[ℹ]  using region ap-southeast-2
[ℹ]  combined include rules: prow-1
[ℹ]  1 nodegroup (prow-1) was included (based on the include/exclude rules)
[ℹ]  will delete 1 nodegroups from auth ConfigMap in cluster "prow-dev"
[!]  removing nodegroup from auth ConfigMap: instance identity ARN "arn:aws:iam::928655657136:role/eksctl-prow-dev-nodegroup-prow-1-NodeInstanceRole-1UFBFQ9Q5BFN1" not found in auth ConfigMap
[ℹ]  will drain 1 nodegroup(s) in cluster "prow-dev"
[ℹ]  cordon node "ip-192-168-4-247.ap-southeast-2.compute.internal"
[ℹ]  cordon node "ip-192-168-45-255.ap-southeast-2.compute.internal"
[!]  ignoring DaemonSet-managed Pods: kube-system/aws-node-t9mrd, kube-system/kube-proxy-tggtw
[!]  ignoring DaemonSet-managed Pods: kube-system/aws-node-lc6f5, kube-system/kube-proxy-kxmzh
[!]  ignoring DaemonSet-managed Pods: kube-system/aws-node-t9mrd, kube-system/kube-proxy-tggtw
[!]  ignoring DaemonSet-managed Pods: kube-system/aws-node-lc6f5, kube-system/kube-proxy-kxmzh
[✔]  drained nodes: [ip-192-168-4-247.ap-southeast-2.compute.internal ip-192-168-45-255.ap-southeast-2.compute.internal]
[ℹ]  will delete 1 nodegroups from cluster "prow-dev"
[ℹ]  1 task: { delete nodegroup "prow-1" [async] }
[ℹ]  will delete stack "eksctl-prow-dev-nodegroup-prow-1"
[✔]  deleted 1 nodegroup(s) from cluster "prow-dev"

#+end_src
* Creating a managed nodegroup
[[https://eksctl.io/usage/eks-managed-nodes/][EKS - Creating a cluster]]
#+begin_src shell
eksctl create nodegroup -f eksctl.yaml
#+end_src

#+RESULTS:
#+begin_example
#+end_example

* go get go
#+begin_src shell
  curl -L https://dl.google.com/go/go1.14.2.linux-amd64.tar.gz | sudo tar -C /usr/local -xzf -
#+end_src

#+RESULTS:
#+begin_example
#+end_example

* hook up

#+begin_src shell :prologue "export PATH=/usr/local/go/bin:$PATH\n"
  echo $PATH
  go get -u k8s.io/test-infra/experiment/add-hook
  add-hook
#+end_src

#+RESULTS:
#+begin_example
#+end_example

#+begin_src shell :prologue "export PATH=/usr/local/go/bin:$PATH\n"
  add-hook
#+end_src

#+RESULTS:
#+begin_example
#+end_example

#+begin_src shell :dir "~/test-infra"
  (
  bazel run //experiment/add-hook -- \
    --hmac-path=../prow-config/.secret-hook \
    --github-token-path=../prow-config/.secret-oauth \
    --hook-url http://prow.cncf.io/hook \
    --repo cncf/k8s-conformance \
    --repo cncf/apisnoop \
#    --repo cncf-infra \
    --confirm=false  # Remove =false to actually add hook
  ) 2>&1
  :
#+end_src

#+RESULTS:
#+begin_example
Starting local Bazel server and connecting to it...
Loading:
Loading: 0 packages loaded
Loading: 0 packages loaded
Analyzing: target //experiment/add-hook:add-hook (1 packages loaded, 0 targets configured)
Analyzing: target //experiment/add-hook:add-hook (31 packages loaded, 776 targets configured)
Analyzing: target //experiment/add-hook:add-hook (31 packages loaded, 6493 targets configured)
Analyzing: target //experiment/add-hook:add-hook (229 packages loaded, 7259 targets configured)
Analyzing: target //experiment/add-hook:add-hook (352 packages loaded, 8692 targets configured)
Analyzing: target //experiment/add-hook:add-hook (591 packages loaded, 10132 targets configured)
INFO: Analyzed target //experiment/add-hook:add-hook (597 packages loaded, 10309 targets configured).
INFO: Found 1 target...
INFO: Deleting stale sandbox base /newhome/ii/.cache/bazel/_bazel_ii/8dad4840a73c734ffda8c8c7e2d452a8/sandbox
[1 / 547] [Prepa] BazelWorkspaceStatusAction stable-status.txt
[19 / 768] Compiling external/com_google_protobuf/src/google/protobuf/stubs/strutil.cc [for host]; 1s linux-sandbox ... (8 actions running)
[28 / 768] Compiling external/com_google_protobuf/src/google/protobuf/generated_message_table_driven_lite.cc [for host]; 2s linux-sandbox ... (8 actions running)
[81 / 768] Compiling external/com_google_protobuf/src/google/protobuf/generated_message_table_driven_lite.cc [for host]; 4s linux-sandbox ... (8 actions running)
[87 / 768] Compiling external/com_google_protobuf/src/google/protobuf/compiler/objectivec/objectivec_enum.cc [for host]; 2s linux-sandbox ... (8 actions running)
[93 / 768] Compiling external/com_google_protobuf/src/google/protobuf/descriptor.cc [for host]; 4s linux-sandbox ... (8 actions running)
[101 / 768] Compiling external/com_google_protobuf/src/google/protobuf/descriptor.cc [for host]; 8s linux-sandbox ... (8 actions running)
[109 / 768] Compiling external/com_google_protobuf/src/google/protobuf/descriptor.cc [for host]; 11s linux-sandbox ... (8 actions running)
[126 / 768] Compiling external/com_google_protobuf/src/google/protobuf/type.pb.cc [for host]; 3s linux-sandbox ... (8 actions running)
[155 / 768] Compiling external/com_google_protobuf/src/google/protobuf/compiler/parser.cc [for host]; 4s linux-sandbox ... (8 actions running)
[166 / 768] Compiling external/com_google_protobuf/src/google/protobuf/compiler/js/js_generator.cc [for host]; 7s linux-sandbox ... (8 actions, 7 running)
[187 / 768] Compiling external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_file.cc [for host]; 2s linux-sandbox ... (8 actions, 7 running)
[203 / 768] Compiling external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_message.cc [for host]; 8s linux-sandbox ... (8 actions, 7 running)
[228 / 768] Compiling external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_string_field.cc [for host]; 4s linux-sandbox ... (8 actions, 7 running)
[264 / 946] Compiling external/com_google_protobuf/src/google/protobuf/compiler/js/js_generator.cc; 7s linux-sandbox ... (8 actions, 7 running)
[295 / 946] Compiling external/com_google_protobuf/src/google/protobuf/descriptor.cc; 12s linux-sandbox ... (8 actions, 7 running)
[341 / 946] Compiling external/com_google_protobuf/src/google/protobuf/compiler/objectivec/objectivec_oneof.cc; 2s linux-sandbox ... (8 actions, 7 running)
[385 / 946] Compiling external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_message.cc; 9s linux-sandbox ... (8 actions, 7 running)
INFO: From Generating Descriptor Set proto_library @go_googleapis//google/iam/v1:iam_proto:
google/iam/v1/options.proto:20:1: warning: Import google/api/annotations.proto is unused.
google/iam/v1/policy.proto:21:1: warning: Import google/api/annotations.proto is unused.
Target //experiment/add-hook:add-hook up-to-date:
  bazel-bin/experiment/add-hook/linux_amd64_stripped/add-hook
INFO: Elapsed time: 134.738s, Critical Path: 17.46s
INFO: 373 processes: 373 linux-sandbox.
INFO: Build completed successfully, 376 total actions
INFO: Running command line: bazel-bin/experiment/add-hook/linux_amd64_stripped/add-hook '--hmac-path=../prow-config/.secret-hook' '--github-token-path=../prow-config/.secret-oauth' --hook-url http://prow.cncf.io/hook --repo cncf/k8s-conformance --repo cncf/apisnoop
INFO: Build completed successfully, 376 total actions
time="2020-05-13T16:56:19Z" level=warning msg="It doesn't look like you are using ghproxy to cache API calls to GitHub! This has become a required component of Prow and other components will soon be allowed to add features that may rapidly consume API ratelimit without caching. Starting May 1, 2020 use Prow components without ghproxy at your own risk! https://github.com/kubernetes/test-infra/tree/master/ghproxy#ghproxy"
time="2020-05-13T16:56:19Z" level=fatal msg="Could not create github client: start ../prow-config/.secret-oauth: error reading ../prow-config/.secret-oauth: open ../prow-config/.secret-oauth: no such file or directory"
/bin/bash: line 9: --confirm=false: command not found
#+end_example
* Adding more repos to prow
- The new repo will need to be defined in the hook above, but also added to plugins
** content of plugins.yaml showing cncf/k8s-conformance added
#+begin_src  shell
  cat plugins.yaml
#+end_src

#+RESULTS:
#+begin_example
# plugin-specific config

# config-updater
# update prow cluster's configmaps from the repo with this plugin enabled; assumed to be a single repo
config_updater:
  maps:
    config.yaml:
      name: config
    plugins.yaml:
      name: plugins
    jobs/**/*.yaml:
      name: job-config

# which plugins should be enabled for which orgs or org/repos
plugins:
  cncf-infra:
  # - approve
  - assign
  - cat
  - dog
  - hold
  - label
  - lgtm
  # - owners-label
  - pony
  - shrug
  - size
  - skip
  - trigger
  - wip
  # - verify-owners
  - yuks

  cncf-infra/prow-config:
  - config-updater

  cncf/k8s-conformance:
  # - approve
  - assign
  - cat
  - dog
  - hold
  - label
  - lgtm
  # - owners-label
  - pony
  - shrug
  - size
  - skip
  - trigger
  - wip
  # - verify-owners
  - yuks
#+end_example

- After updating plugins run the following to apply it it the cluster.
** Lets apply the change
#+begin_src  shell
  kubectl create configmap plugins --from-file=plugins.yaml=./plugins.yaml  --dry-run -o yaml | kubectl replace configmap plugins -f -
#+end_src

#+RESULTS:
#+begin_example
configmap/plugins replaced
#+end_example


* ghproxy
#+begin_src shell
  kubectl apply -f manifests/ghproxy.yaml
#+end_src

#+RESULTS:
#+begin_example
persistentvolumeclaim/ghproxy created
deployment.apps/ghproxy created
service/ghproxy created
#+end_example

* Footnotes
** software
*** direnv
*** aws-iam-authenticator
https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html
** gotchas
*** documentation seems to call it the oauth secret.... when in fact it's a github personal access tokens
*** cluster authentication / iam
 https://github.com/kubernetes-sigs/aws-iam-authenticator/issues/174#issuecomment-450651720

*** cluster-admin role
 #+BEGIN_SRC sh
   kubectl get clusterrolebinding cluster-admin -o yaml
 #+END_SRC

 #+RESULTS:
 #+begin_src sh
 apiVersion: rbac.authorization.k8s.io/v1
 kind: ClusterRoleBinding
 metadata:
   annotations:
     rbac.authorization.kubernetes.io/autoupdate: "true"
   creationTimestamp: "2020-04-06T04:19:41Z"
   labels:
     kubernetes.io/bootstrapping: rbac-defaults
   name: cluster-admin
   resourceVersion: "95"
   selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/cluster-admin
   uid: c8c1eb3a-72a4-45d3-8ae2-c7d8abda71ee
 roleRef:
   apiGroup: rbac.authorization.k8s.io
   kind: ClusterRole
   name: cluster-admin
 subjects:
 - apiGroup: rbac.authorization.k8s.io
   kind: Group
   name: system:masters
 #+end_src
** ENV for aws cli
 https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html

 **AWS_PROFILE**
