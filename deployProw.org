# This documents deploying prow to an empty cluster, this was built using the /infra/modules/aws-modules terraform build. The intent is that it will work with all builds

* Confirm cluster aws cli and kubeconfig points to the right cluster
** List all eks clusters
#+begin_src  shell
aws eks list-clusters
#+end_src

#+RESULTS:
#+BEGIN_example
---------------------
|   ListClusters    |
+-------------------+
||    clusters     ||
|+-----------------+|
||  prow-dev       ||
||  prow-LltO5Bjk  ||
||  prow-stg       ||
||  prow-ztwaOEOw  ||
|+-----------------+|
#+END_example

** Set current context to be the newly created cluster
#+begin_src shell
  aws sts get-caller-identity
#+end_src

#+RESULTS:
#+BEGIN_example
----------------------------------------------------------
|                    GetCallerIdentity                   |
+---------+----------------------------------------------+
|  Account|  928655657136                                |
|  Arn    |  arn:aws:iam::928655657136:user/zz@ii.coop   |
|  UserId |  AIDA5QOBQZCYJLLBQL4OI                       |
+---------+----------------------------------------------+
#+END_example

# Set current context to be the newly created cluster
#+begin_src shell
  aws eks update-kubeconfig --name prow-1QQTdZBm --region ap-southeast-2
#+end_src

#+RESULTS:
#+begin_example
Updated context arn:aws:eks:ap-southeast-2:928655657136:cluster/prow-1QQTdZBm in /home/ii/.kube/config
#+end_example

#+begin_src shell
 kubectl config view --minify
#+end_src
#+RESULTS:d
#+begin_example
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://7EC74CD0AF19F532E3384523191552E5.sk1.ap-southeast-2.eks.amazonaws.com
  name: arn:aws:eks:ap-southeast-2:928655657136:cluster/prow-1QQTdZBm
contexts:
- context:
    cluster: arn:aws:eks:ap-southeast-2:928655657136:cluster/prow-1QQTdZBm
    user: arn:aws:eks:ap-southeast-2:928655657136:cluster/prow-1QQTdZBm
  name: arn:aws:eks:ap-southeast-2:928655657136:cluster/prow-1QQTdZBm
current-context: arn:aws:eks:ap-southeast-2:928655657136:cluster/prow-1QQTdZBm
kind: Config
preferences: {}
users:
- name: arn:aws:eks:ap-southeast-2:928655657136:cluster/prow-1QQTdZBm
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      args:
      - --region
      - ap-southeast-2
      - eks
      - get-token
      - --cluster-name
      - prow-1QQTdZBm
      command: aws
      env:
      - name: AWS_PROFILE
        value: prow
#+end_example

** Add certificates for https
#+begin_src yaml :tangle manifests/cert.yaml
apiVersion: cert-manager.io/v1beta1
kind: Certificate
metadata:
  name: letsencrypt-prod-prow-cncf-io
spec:
  secretName: letsencrypt-prod-prow-cncf-io
  issuerRef:
    name: letsencrypt-prod-prow-cncf-io
    kind: ClusterIssuer
    group: cert-manager.io
  dnsNames:
    - 'prow.cncf.io'
#+end_src


#+begin_src yaml :tangle manifests/cluster-issuer.yaml
apiVersion: cert-manager.io/v1beta1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod-prow-cncf-io
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email:
    privateKeySecretRef:
      name: letsencrypt-prod-prow-cncf-io
    solvers:
      - http01:
          ingress:
            class: nginx
        selector:
          dnsNames:
            - "prow.cncf.io"
#+end_src

#+begin_src shell
kubectl apply -f manifests/cluster-issuer.yaml -f manifests/cert.yaml
#+end_src

#+RESULTS:
#+begin_example
clusterissuer.cert-manager.io/letsencrypt-prod-prow-cncf-io created
certificate.cert-manager.io/letsencrypt-prod-prow-cncf-io configured
#+end_example

#+begin_src shell
  kubectl get certs
#+end_src

#+RESULTS:
#+begin_example
NAME                            READY   SECRET                          AGE
letsencrypt-prod-prow-cncf-io   False   letsencrypt-prod-prow-cncf-io   11m
#+end_example

* Add github secrets, to the cluster
These secrets gets generated in github, we manually add them to local file system for use here
 TODO: document exact process for getting .secrets-hook and .secret-oauth
** github-hmac / hook
 #+begin_src shell
   kubectl delete secret hmac-token
   kubectl create secret generic hmac-token --from-file=hmac=.secret-hook
 #+end_src

 #+RESULTS:
 #+begin_example
 secret "hmac-token" deleted
 secret/hmac-token created
 #+end_example

** github-oauth
 #+begin_src shell
   kubectl delete secret oauth-token
   kubectl create secret generic oauth-token --from-file=oauth=.secret-oauth
 #+end_src

 #+RESULTS:
 #+begin_example
 secret "oauth-token" deleted
 secret/oauth-token created
 #+end_example

* Install Prow components manifst
** cluster/starter.yaml
https://github.com/kubernetes/test-infra/blob/master/prow/getting_started_deploy.md#add-the-prow-components-to-the-cluster
#+begin_src shell :dir "~/prow-config"
  kubectl apply -f manifests/starter.yaml
#+end_src

#+RESULTS:
#+begin_example
configmap/plugins unchanged
configmap/config unchanged
customresourcedefinition.apiextensions.k8s.io/prowjobs.prow.k8s.io unchanged
deployment.apps/hook unchanged
service/hook unchanged
deployment.apps/plank unchanged
deployment.apps/sinker unchanged
deployment.apps/deck unchanged
service/deck unchanged
deployment.apps/horologium unchanged
deployment.apps/tide unchanged
service/tide unchanged
ingress.extensions/ing configured
deployment.apps/statusreconciler unchanged
namespace/test-pods unchanged
serviceaccount/deck unchanged
rolebinding.rbac.authorization.k8s.io/deck unchanged
rolebinding.rbac.authorization.k8s.io/deck unchanged
role.rbac.authorization.k8s.io/deck unchanged
role.rbac.authorization.k8s.io/deck unchanged
serviceaccount/horologium unchanged
role.rbac.authorization.k8s.io/horologium unchanged
rolebinding.rbac.authorization.k8s.io/horologium unchanged
serviceaccount/plank unchanged
role.rbac.authorization.k8s.io/plank unchanged
role.rbac.authorization.k8s.io/plank unchanged
rolebinding.rbac.authorization.k8s.io/plank unchanged
rolebinding.rbac.authorization.k8s.io/plank unchanged
serviceaccount/sinker unchanged
role.rbac.authorization.k8s.io/sinker unchanged
role.rbac.authorization.k8s.io/sinker unchanged
rolebinding.rbac.authorization.k8s.io/sinker unchanged
rolebinding.rbac.authorization.k8s.io/sinker unchanged
serviceaccount/hook unchanged
role.rbac.authorization.k8s.io/hook unchanged
rolebinding.rbac.authorization.k8s.io/hook unchanged
serviceaccount/tide unchanged
role.rbac.authorization.k8s.io/tide unchanged
rolebinding.rbac.authorization.k8s.io/tide unchanged
serviceaccount/statusreconciler unchanged
role.rbac.authorization.k8s.io/statusreconciler unchanged
rolebinding.rbac.authorization.k8s.io/statusreconciler unchanged
#+end_example
* Verify components
** services
#+begin_src shell
  kubectl get services
#+end_src

#+RESULTS:
#+begin_example
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE
deck         NodePort    172.20.235.175   <none>        80:30203/TCP      11s
ghproxy      ClusterIP   172.20.85.71     <none>        80/TCP,9090/TCP   18m
hook         NodePort    172.20.221.251   <none>        8888:32389/TCP    11s
kubernetes   ClusterIP   172.20.0.1       <none>        443/TCP           4h3m
tide         NodePort    172.20.30.191    <none>        80:32681/TCP      11s
#+end_example

** pods
#+begin_src shell
  kubectl get pods
#+end_src

#+RESULTS:
#+begin_example
NAME                                READY   STATUS    RESTARTS   AGE
deck-7d486fcc-pssgl                 1/1     Running   0          13s
deck-7d486fcc-rqglq                 1/1     Running   0          13s
ghproxy-5ccfb97b79-5prr9            1/1     Running   0          18m
hook-5674b4dc6b-7qtvv               1/1     Running   0          13s
hook-5674b4dc6b-khkn5               0/1     Running   0          14s
horologium-6947d84b-8spsr           1/1     Running   0          14s
plank-569bd9857d-bs62q              1/1     Running   0          14s
sinker-5bd5749656-bbrmd             1/1     Running   0          14s
statusreconciler-64d56987cc-8z2tb   1/1     Running   0          14s
tide-7f89d88467-pvpc5               1/1     Running   0          14s
#+end_example

** deployment

#+begin_src shell
  kubectl get deployments
#+end_src

#+RESULTS:
#+begin_example
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
deck               2/2     2            2           21s
ghproxy            1/1     1            1           18m
hook               2/2     2            2           21s
horologium         1/1     1            1           21s
plank              1/1     1            1           21s
sinker             1/1     1            1           21s
statusreconciler   1/1     1            1           21s
tide               1/1     1            1           21s
#+end_example

** ingress
#+begin_src shell
  kubectl get ingress
#+end_src

#+RESULTS:
#+begin_example
NAME   HOSTS          ADDRESS                                                                        PORTS     AGE
ing    prow.cncf.io   a6db92a5df19741c8a43dc8aa8e486e2-1450765144.ap-southeast-2.elb.amazonaws.com   80, 443   57s
#+end_example

#+begin_src shell
  kubectl get ingress ing -o yaml
#+end_src

#+RESULTS:
#+begin_example
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod-prow-cncf-io
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{"cert-manager.io/cluster-issuer":"letsencrypt-prod-prow-cncf-io","kubernetes.io/ingress.class":"nginx","nginx.ingress.kubernetes.io/rewrite-target":"/"},"name":"ing","namespace":"default"},"spec":{"backend":{"serviceName":"deck","servicePort":80},"rules":[{"host":"prow.cncf.io","http":{"paths":[{"backend":{"serviceName":"deck","servicePort":80},"path":"/"},{"backend":{"serviceName":"hook","servicePort":8888},"path":"/hook"}]}}],"tls":[{"hosts":["prow.cncf.io"],"secretName":"letsencrypt-prod-prow-cncf-io"}]}}
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
  creationTimestamp: "2020-08-13T02:41:27Z"
  generation: 2
  name: ing
  namespace: default
  resourceVersion: "33185"
  selfLink: /apis/extensions/v1beta1/namespaces/default/ingresses/ing
  uid: 224e72ef-d436-4879-a83c-0b57852fdc47
spec:
  backend:
    serviceName: deck
    servicePort: 80
  rules:
  - host: prow.cncf.io
    http:
      paths:
      - backend:
          serviceName: deck
          servicePort: 80
        path: /
      - backend:
          serviceName: hook
          servicePort: 8888
        path: /hook
  tls:
  - hosts:
    - prow.cncf.io
    secretName: letsencrypt-prod-prow-cncf-io
status:
  loadBalancer:
    ingress:
    - hostname: a6db92a5df19741c8a43dc8aa8e486e2-1450765144.ap-southeast-2.elb.amazonaws.com
#+end_example

* Go get hook
* Adding more repos to prow
- The new repo will need to be defined in the hook above, but also added to plugins

** Lets apply the change
#+begin_src  shell
  kubectl create configmap plugins --from-file=plugins.yaml=./plugins.yaml  --dry-run -o yaml | kubectl replace configmap plugins -f -
#+end_src

#+RESULTS:
#+begin_example
configmap/plugins replaced
#+end_example

* ghproxy
#+begin_src shell
  kubectl apply -f manifests/ghproxy.yaml
#+end_src

#+RESULTS:
#+begin_example
persistentvolumeclaim/ghproxy unchanged
deployment.apps/ghproxy unchanged
service/ghproxy unchanged
#+end_example

* hook up
   For this to work, you will need to make sure the hook is added on the github side, you have to whitelist the hook-url in github settings, it also require

#+begin_src shell :dir "~/test-infra/"
  ./bazel-bin/experiment/update-hook/linux_amd64_stripped/update-hook '--github-endpoint=http://ghproxy/' '--github-token-path=~/prow-config/.secret-oauth' '--hmac-path=../prow-config/.secret-hook' --hook-url http://adc0c1d070fdb46b2897a567e5c017db-1395387388.ap-southeast-2.elb.amazonaws.com/hook --repo cncf-infra/k8s-conformance --repo cncf-infra/prow-config
#+end_src

#+RESULTS:
#+begin_example
#+end_example

* Deploy verify conformance release/test external plugins
** loading config map for vcr.yaml
   #+begin_src shell :dir ~/prow-config
     kubectl delete configmap vcr-config
     kubectl create configmap vcr-config --from-file=prow/external-plugins/verify-conformance-release/vcr.yaml
   #+end_src

   #+RESULTS:
   #+begin_example
   configmap/vcr-config created
   #+end_example

** apply verify-conformance-deployment.yaml
   #+begin_src shell :dir "~/prow-config"
     kubectl apply -f manifests/verify-conformance-release-deployment.yaml
   #+end_src

   #+RESULTS:
   #+begin_example
   deployment.apps/verify-conformance-release created
   #+end_example

** loading config map for vct.yaml
   #+begin_src shell :dir ~/prow-config
     kubectl delete configmap vct-config
     kubectl create configmap vct-config --from-file=prow/external-plugins/verify-conformance-tests/vct.yaml
   #+end_src

   #+RESULTS:
   #+begin_example
   configmap/vct-config created
   #+end_example

** apply verify-conformance-deployment.yaml
   #+begin_src shell :dir "~/prow-config"
    kubectl apply -f manifests/verify-conformance-test-deployment.yaml
   #+end_src

   #+RESULTS:
   #+begin_example
   deployment.apps/verify-conformance-test created
   #+end_example
