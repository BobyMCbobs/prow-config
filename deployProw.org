# This documents deploying prow to an empty cluster, this was built using the /infra/modules/aws-modules terraform build. The intent is that it will work with all builds

* Confirm cluster aws cli and kubeconfig points to the right cluster
** List all eks clusters
#+begin_src  shell
aws eks list-clusters
#+end_src

#+RESULTS:
#+begin_example
{
    "clusters": [
        "prow-dev",
        "prowManual",
        "prow-1QQTdZBm",
        "prow-stg"
    ]
}
#+end_example

** Set current context to be the newly created cluster
#+begin_src shell
  aws sts get-caller-identity
#+end_src

#+RESULTS:
#+begin_example
{
    "UserId": "AIDA5QOBQZCYOSBXDQBV2",
    "Account": "928655657136",
    "Arn": "arn:aws:iam::928655657136:user/prow.cncf.io"
}
#+end_example

# Set current context to be the newly created cluster
#+begin_src shell
  aws eks update-kubeconfig --name prow-1QQTdZBm --region ap-southeast-2
#+end_src

#+RESULTS:
#+begin_example
Updated context arn:aws:eks:ap-southeast-2:928655657136:cluster/prow-1QQTdZBm in /home/ii/.kube/config
#+end_example

#+begin_src shell
 kubectl config view --minify
#+end_src
#+RESULTS:d
#+begin_example
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://7EC74CD0AF19F532E3384523191552E5.sk1.ap-southeast-2.eks.amazonaws.com
  name: arn:aws:eks:ap-southeast-2:928655657136:cluster/prow-1QQTdZBm
contexts:
- context:
    cluster: arn:aws:eks:ap-southeast-2:928655657136:cluster/prow-1QQTdZBm
    user: arn:aws:eks:ap-southeast-2:928655657136:cluster/prow-1QQTdZBm
  name: arn:aws:eks:ap-southeast-2:928655657136:cluster/prow-1QQTdZBm
current-context: arn:aws:eks:ap-southeast-2:928655657136:cluster/prow-1QQTdZBm
kind: Config
preferences: {}
users:
- name: arn:aws:eks:ap-southeast-2:928655657136:cluster/prow-1QQTdZBm
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      args:
      - --region
      - ap-southeast-2
      - eks
      - get-token
      - --cluster-name
      - prow-1QQTdZBm
      command: aws
      env:
      - name: AWS_PROFILE
        value: prow
#+end_example

** Add certificates for https
#+begin_src yaml :tangle manifests/cert.yaml
apiVersion: cert-manager.io/v1beta1
kind: Certificate
metadata:
  name: letsencrypt-prod-prow-cncf-io
spec:
  secretName: letsencrypt-prod-prow-cncf-io
  issuerRef:
    name: letsencrypt-prod-prow-cncf-io
    kind: ClusterIssuer
    group: cert-manager.io
  dnsNames:
    - 'prow.cncf.io'
#+end_src


#+begin_src yaml :tangle manifests/cluster-issuer.yaml
apiVersion: cert-manager.io/v1beta1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod-prow-cncf-io
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email:
    privateKeySecretRef:
      name: letsencrypt-prod-prow-cncf-io
    solvers:
      - http01:
          ingress:
            class: nginx
        selector:
          dnsNames:
            - "prow.cncf.io"
#+end_src

#+begin_src shell
kubectl apply -f manifests/cluster-issuer.yaml -f manifests/cert.yaml
#+end_src

#+RESULTS:
#+begin_example
clusterissuer.cert-manager.io/letsencrypt-prod-prow-cncf-io created
certificate.cert-manager.io/letsencrypt-prod-prow-cncf-io created
#+end_example

* Add github secrets, to the cluster
These secrets gets generated in github, we manually add them to local file system for use here
 TODO: document exact process for getting .secrets-hook and .secret-oauth
** github-hmac / hook
 #+begin_src shell
   kubectl delete secret hmac-token
   kubectl create secret generic hmac-token --from-file=hmac=.secret-hook
 #+end_src

 #+RESULTS:
 #+begin_example
 secret/hmac-token created
 #+end_example

** github-oauth
 #+begin_src shell
   kubectl delete secret oauth-token
   kubectl create secret generic oauth-token --from-file=oauth=.secret-oauth
 #+end_src

 #+RESULTS:
 #+begin_example
 secret/oauth-token created
 #+end_example

* Install Prow components manifst
** cluster/starter.yaml
https://github.com/kubernetes/test-infra/blob/master/prow/getting_started_deploy.md#add-the-prow-components-to-the-cluster
#+begin_src shell :dir "~/prow-config"
  kubectl apply -f manifests/starter.yaml
#+end_src

#+RESULTS:
#+begin_example
configmap/plugins created
configmap/config created
customresourcedefinition.apiextensions.k8s.io/prowjobs.prow.k8s.io created
deployment.apps/hook created
service/hook created
deployment.apps/plank created
deployment.apps/sinker created
deployment.apps/deck created
service/deck created
deployment.apps/horologium created
deployment.apps/tide created
service/tide created
ingress.extensions/ing created
deployment.apps/statusreconciler created
namespace/test-pods created
serviceaccount/deck created
rolebinding.rbac.authorization.k8s.io/deck created
rolebinding.rbac.authorization.k8s.io/deck created
role.rbac.authorization.k8s.io/deck created
role.rbac.authorization.k8s.io/deck created
serviceaccount/horologium created
role.rbac.authorization.k8s.io/horologium created
rolebinding.rbac.authorization.k8s.io/horologium created
serviceaccount/plank created
role.rbac.authorization.k8s.io/plank created
role.rbac.authorization.k8s.io/plank created
rolebinding.rbac.authorization.k8s.io/plank created
rolebinding.rbac.authorization.k8s.io/plank created
serviceaccount/sinker created
role.rbac.authorization.k8s.io/sinker created
role.rbac.authorization.k8s.io/sinker created
rolebinding.rbac.authorization.k8s.io/sinker created
rolebinding.rbac.authorization.k8s.io/sinker created
serviceaccount/hook created
role.rbac.authorization.k8s.io/hook created
rolebinding.rbac.authorization.k8s.io/hook created
serviceaccount/tide created
role.rbac.authorization.k8s.io/tide created
rolebinding.rbac.authorization.k8s.io/tide created
serviceaccount/statusreconciler created
role.rbac.authorization.k8s.io/statusreconciler created
rolebinding.rbac.authorization.k8s.io/statusreconciler created
#+end_example
* Verify components
** services
#+begin_src shell
  kubectl get services
#+end_src

#+RESULTS:
#+begin_example
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE
deck         NodePort    172.20.96.68    <none>        80:32347/TCP      18m
ghproxy      ClusterIP   172.20.142.74   <none>        80/TCP,9090/TCP   16m
hook         NodePort    172.20.171.32   <none>        8888:30856/TCP    18m
kubernetes   ClusterIP   172.20.0.1      <none>        443/TCP           4h23m
tide         NodePort    172.20.171.4    <none>        80:32767/TCP      18m
#+end_example

** pods
#+begin_src shell
  kubectl get pods
#+end_src

#+RESULTS:
#+begin_example
NAME                                          READY   STATUS    RESTARTS   AGE
deck-7d486fcc-2flgz                           1/1     Running   0          18m
deck-7d486fcc-82bvg                           1/1     Running   0          18m
ghproxy-5ccfb97b79-mqffj                      1/1     Running   0          16m
hook-5674b4dc6b-4pnpz                         1/1     Running   1          18m
hook-5674b4dc6b-vdhzt                         1/1     Running   1          18m
horologium-6947d84b-xv445                     1/1     Running   0          18m
plank-569bd9857d-sgvxk                        1/1     Running   0          18m
sinker-5bd5749656-6fhsh                       1/1     Running   0          18m
statusreconciler-64d56987cc-ftw6b             1/1     Running   0          18m
tide-7f89d88467-bk4k2                         1/1     Running   0          18m
verify-conformance-release-7468db6458-n7mq6   1/1     Running   0          9m50s
verify-conformance-test-c76f6656d-j8vvm       1/1     Running   0          9m27s
#+end_example

** deployment

#+begin_src shell
  kubectl get deployments
#+end_src

#+RESULTS:
#+begin_example
NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
deck                         2/2     2            2           18m
ghproxy                      1/1     1            1           16m
hook                         2/2     2            2           18m
horologium                   1/1     1            1           18m
plank                        1/1     1            1           18m
sinker                       1/1     1            1           18m
statusreconciler             1/1     1            1           18m
tide                         1/1     1            1           18m
verify-conformance-release   1/1     1            1           9m55s
verify-conformance-test      1/1     1            1           9m32s
#+end_example

** ingress
#+begin_src shell
  kubectl get ingress
#+end_src

#+RESULTS:
#+begin_example
NAME   HOSTS          ADDRESS                                                                       PORTS     AGE
ing    prow.cncf.io   a531613ee6e6146fa8cbdc6fdb95a885-385522534.ap-southeast-2.elb.amazonaws.com   80, 443   18m
#+end_example

#+begin_src shell
  kubectl get ingress ing -o yaml
#+end_src

#+RESULTS:
#+begin_example
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod-prow-cncf-io
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{"cert-manager.io/cluster-issuer":"letsencrypt-prod-prow-cncf-io","kubernetes.io/ingress.class":"nginx","nginx.ingress.kubernetes.io/rewrite-target":"/"},"name":"ing","namespace":"default"},"spec":{"backend":{"serviceName":"deck","servicePort":80},"rules":[{"host":"prow.cncf.io","http":{"paths":[{"backend":{"serviceName":"deck","servicePort":80},"path":"/"},{"backend":{"serviceName":"hook","servicePort":8888},"path":"/hook"}]}}],"tls":[{"hosts":["prow.cncf.io"],"secretName":"letsencrypt-prod-prow-cncf-io"}]}}
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
  creationTimestamp: "2020-08-12T02:23:22Z"
  generation: 1
  name: ing
  namespace: default
  resourceVersion: "25077"
  selfLink: /apis/extensions/v1beta1/namespaces/default/ingresses/ing
  uid: d06eee37-ba35-4ad9-b14e-1beb6e590410
spec:
  backend:
    serviceName: deck
    servicePort: 80
  rules:
  - host: prow.cncf.io
    http:
      paths:
      - backend:
          serviceName: deck
          servicePort: 80
        path: /
      - backend:
          serviceName: hook
          servicePort: 8888
        path: /hook
  tls:
  - hosts:
    - prow.cncf.io
    secretName: letsencrypt-prod-prow-cncf-io
status:
  loadBalancer:
    ingress:
    - hostname: a531613ee6e6146fa8cbdc6fdb95a885-385522534.ap-southeast-2.elb.amazonaws.com
#+end_example

* Go get hook
* Adding more repos to prow
- The new repo will need to be defined in the hook above, but also added to plugins

** Lets apply the change
#+begin_src  shell
  kubectl create configmap plugins --from-file=plugins.yaml=./plugins.yaml  --dry-run -o yaml | kubectl replace configmap plugins -f -
#+end_src

#+RESULTS:
#+begin_example
configmap/plugins replaced
#+end_example

* ghproxy
#+begin_src shell
  kubectl apply -f manifests/ghproxy.yaml
#+end_src

#+RESULTS:
#+begin_example
persistentvolumeclaim/ghproxy created
deployment.apps/ghproxy created
service/ghproxy created
#+end_example

* hook up
   For this to work, you will need to make sure the hook is added on the github side, you have to whitelist the hook-url in github settings, it also require

#+begin_src shell :dir "~/test-infra/"
  ./bazel-bin/experiment/update-hook/linux_amd64_stripped/update-hook '--github-endpoint=http://ghproxy/' '--github-token-path=/home/ii/prow-config/.secret-oauth' '--hmac-path=../prow-config/.secret-hook' --hook-url http://adc0c1d070fdb46b2897a567e5c017db-1395387388.ap-southeast-2.elb.amazonaws.com/hook --repo cncf-infra/k8s-conformance --repo cncf-infra/prow-config
#+end_src

#+RESULTS:
#+begin_example
#+end_example

* Deploy verify conformance release/test external plugins
** loading config map for vcr.yaml
   #+begin_src shell :dir ~/prow-config
     kubectl delete configmap vcr-config
     kubectl create configmap vcr-config --from-file=prow/external-plugins/verify-conformance-release/vcr.yaml
   #+end_src

   #+RESULTS:
   #+begin_example
   configmap "vcr-config" deleted
   configmap/vcr-config created
   #+end_example

** apply verify-conformance-deployment.yaml
   #+begin_src shell :dir "~/prow-config"
     kubectl apply -f manifests/verify-conformance-release-deployment.yaml
   #+end_src

   #+RESULTS:
   #+begin_example
   deployment.apps/verify-conformance-release created
   #+end_example

** loading config map for vct.yaml
   #+begin_src shell :dir ~/prow-config
     kubectl delete configmap vct-config
     kubectl create configmap vct-config --from-file=prow/external-plugins/verify-conformance-tests/vct.yaml
   #+end_src

   #+RESULTS:
   #+begin_example
   configmap/vct-config created
   #+end_example

** apply verify-conformance-deployment.yaml
   #+begin_src shell :dir "~/prow-config"
    kubectl apply -f manifests/verify-conformance-test-deployment.yaml
   #+end_src

   #+RESULTS:
   #+begin_example
   deployment.apps/verify-conformance-test created
   #+end_example
